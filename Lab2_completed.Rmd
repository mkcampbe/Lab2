---
title: "Lab 2: Essential tasks in R"
author: "Mary Kate Campbell"
date: "01/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/UMBio201/Lab2/")
```

# Load packages

Packages in R are basically sets of additional functions that let you do more stuff. The functions you’ve been using so far are part of R (also called base install) and no extra action is needed to utilize these functions; packages give you access to more functions. Before you use a package for the first time you need to install it on your machine. Then any subsequent R sessions where the package will be used require an load/import step at the beginning of the session. The tidyverse package should already be installed on the USB lab computers. This is an "umbrella-package" that installs several packages useful for data analysis which work together well such as tidyr, dplyr, ggplot2, tibble, etc.
 
The tidyverse package tries to address 3 common issues that arise when doing data analysis with some of the functions that come with R:
 
* The results from a base R function sometimes depend on the type of data.
* Using R expressions in a non standard way, which can be confusing for new learners.
* Hidden arguments, having default operations that new learners are not aware of.

There are several other useful packages that are part of the tidyverse or are written to work well with the tidyverse. Install and load these packages: readxl, broom, cowplot. 
```{r Load packages, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
library(tidyverse)
library(readxl)
library(broom)
library(cowplot)
set.seed(7)
```
Notice the set.seed() function. This is important to include if you are using any functions that start from a random number generator. Without set.seed() this could cause different outcomes if the code is re-run at a later time. 


# Import data

There are several methods to read in the SCFA measurement data. We could use the readr package, which is loaded as part of the tidyverse. This package has functions for reading in specific file formats; general delimited files (read_delim()), tab separated value files (read_tsv()), comma separated value files (read_csv()), fixed width files (read_fwf()), and files where columns are separated by whitespace (read_table()). We are also going to use the read_excel() function from the readxl package to read a table in from a Microsoft Excel-formatted spreadsheet since Excel spreadsheets are a common way collaborators share data.

Each of these functions has a decent number of options that default to values that are generally intuitive. Be careful - there are other similarly named functions (e.g., read.tsv()) that are actually part of base R and have somewhat unexpected defaults. What are the defaults of these functions? What options can you change? Remember from the last lesson that you can view the documentation for any function. 

### with readr

The output of the read functions that are part of the tidyverse are a special type of data frame called a tibble. To back up a step, what is a data frame? A data frame can be thought of as a table where each row represents a different entity and each column represents a different aspect of that entity. You can also imagine data frames as multiple vectors put together, where each column is a different vector. 

For example, the scfa_wkly data frame (tibble) variable stores the value of a data frame where each row represents a different person and each column represents various attributes of those people such as their participant identification number, weight, height, etc. Each row has the same number of columns. If a piece of data is missing, then R will denote the value for that entity with the NA value. 
 
There are some special aspects of a tibble to be aware of. Perhaps most important is that there are no names on the rows. Absence of row names is a safety measure to protect you from some weird quirks in R. Another difference is when you enter the name of the data frame at the prompt, instead of having the entire data frame vomited at your screen, you get an abbreviated output.

```{r}
# import the data frame with readr function
txt_df <- read_delim(file = "raw_data/Lab2_data.txt", 
                        delim = "\t", escape_double = FALSE, trim_ws = TRUE, na=c("NA"),
                        col_types = list())
txt_df
``` 

The abbreviated output above gives the first several columns and the first ten rows of the data frame. You’ll notice that at the bottom of the output, it tells us the total number of rows and columns. In addition, the output tells us the variable type of each column.

```{r}
# remove data frame from global environment 
rm(txt_df)
# data frame no longer in list in upper left panel 
```

### with excel files

We'll repeat the import using a different function, and review the output. Typing out the column types isn't a required argumet for read_excel, but it can save time and troubleshooting steps later when a column may have been read in as the incorrect type because it was left up to the computer to guess. 
```{r include=FALSE}
# import an Excel verion of the file with readxl function

exl_df <- read_excel("raw_data/Lab2_data.xlsx",
                     sheet = "individual_measure", 
                     col_names = TRUE, trim_ws = TRUE, 
                     na = c("", "NA", "-----"), 
                     col_types = c("participant_id" = "text", 
                                   "sample_number" = "text", 
                                   "sample_id" = "text", 
                                   "study_week" = "text", 
                                   "semester" = "text",
                                   "use_data" = "text", 
                                   "quantity_compliant" = "text", 
                                   "frequency" = "text", 
                                   "supplement_consumed" = "text", 
                                   "sample_weight_g" = "numeric",
                                   "acetate_mM" = "numeric", 
                                   "butyrate_mM" = "numeric", 
                                   "propionate_mM" = "numeric", 
                                   "scfa_notes" = "text", 
                                   "pH" = "numeric",
                                   "bristol_score" = "numeric")) 

# type into the console: exl_df
```

The output in the console is similar to what we created above, we can see the dimensions (rows and columns) and type of each variable. For example, the tube_wt column contains dbl (double precision) numbers and the Participant_ID column contains chr (character) values. You’ll also notice that zero values have a lighter color and any NAs are red. These features are all meant to improve the visualization of the data.

This format is an easy way to identify a tibble. Tibbles tweak some of the default behaviors of data frame objects. Accessing or modifying individual columns of a tibble or data frame is very similar to the manipulations on vectors. Going forward we will almost exclusively be working with tibbles/data frames. 


# Accessing dataframes 

The functions below can provide useful information about a data frame. When you first import a data frame you likely will want to view it, or part of it. Run each function below on the data frame `exl_df`, and read the help pages if necessary. In the comments briefly describe what each function tells you about the data frame. 

```{r eval=FALSE, include=FALSE}
nrow(exl_df) #tells us the number of rows in the data frame
ncol(exl_df) #tells us the number of columns in the data frame  
dim(exl_df) #tells us the dimensions of the data frame (rows x columns)
colnames(exl_df) #tells us the names of the columns 
rownames(exl_df) #if we had row names, it would tell us the names of the rows like colnames                       tells us the names of the columns
glimpse(exl_df) #shows us every column in a data frame and some of the data in each column,                       shows us the type of vector in each column 
str(exl_df) #gives us a cleaner, compact idea of what the structure of the data frame 
summary(exl_df) #gives us the IQR and the important statistical values of analsysis such as                     mean, min, and max. also tells us how many N/A's are in the column  
```


# dplyr + tidyr

The base install of R uses bracket subsetting to access subsets of data frames. Bracket subsetting can be cumbersome and difficult to read, especially for complicated operations; so we won't use it! Enter dplyr. dplyr is a package for making tabular data manipulation easier. It pairs nicely with tidyr which enables you to swiftly convert between different data formats for plotting and analysis (both of these packages are part of the tidyverse). We’re going to learn some of the most common dplyr functions:
```{r eval=FALSE, include=FALSE}
select() #retain/drop columns
filter() #keep matching or non-matching rows based on conditions
mutate() #create new columns by using information from other columns, or modify exisiting columns
group_by() %>% summarize() #conduct calcuations on grouped data; reduces down number of rows 
arrange() #sort results
count() #count discrete values
```

### Selecting columns

The first argument to this function is the data frame (exl_df), and the subsequent arguments are the names of the columns to keep. To retain columns of a data frame, use select(): 
```{r}
# retains columns named as arguments
select(exl_df, study_week, participant_id, butyrate_mM)
```

To retain all columns except certain ones, put a "-" in front of the variable (column) to exclude it. This will select all the variables in exl_df except use_data and notes:  
```{r}
# drops columns named with hyphen 
select(exl_df, -use_data, -scfa_notes)
```

In both of the examples above we did not assign the modified data frame to a new variable, so the underlying data frame has not been changed. If we would like to save this new subsetted data frame, just use the assignment operator:
```{r}
new_df <- select(exl_df, study_week, participant_id, butyrate_mM)
```

### Filtering rows

To choose or exclude rows based on specific criteria, use filter(). The first arugment is the name of the data frame (exl_df), the next argument is the specific column of the data frame (semester). As you run each example below, notice how the number of rows differs in each result. 
```{r}
filter(exl_df, semester == "Fall2018") # retain matches
```

```{r}
filter(exl_df, scfa_notes != "used average empty weight") # exclude matches
```

```{r}
# can filter for multiple criteria 
filter(exl_df, # first argument is the data frame
       semester == "Fall2018", # next three arguments are the filtering critera separated by commas 
       supplement_consumed == "BRMPS", # each argument is on a new line so it is easy to read 
       butyrate_mM > 1.0) 
```
The combined filtering steps above retain any data from Fall 2018, from individuals who consumed Bob's Red Mill Potato Starch (BRMPS), and indivduals with butyrate concentrations greater than 1.0 mM. These series of arguments return data drom individuals who match all three of these conditions. 

```{r}
# matches one of listed options: | equivent to "OR"
filter(exl_df, semester == "Fall2018" | semester == "Fall2019") 
```

```{r}
# matches both/all options: & equivent to "AND"
filter(exl_df, 
       semester == "Fall2018" & supplement_consumed == "BRMPS") 
```

### Pipes

What if you want to select and filter at the same time? There are three ways to do this: 

* use intermediate steps
* nested functions
* pipes

With intermediate steps, you create a temporary data frame and use that as input to the next function.
```{r}
# filter and save result to new data frame 
scfa_filtered <- filter(exl_df, sample_weight_g > 0.10)
# do something with data frame just created, and save it to another new one
but_data <- select(scfa_filtered, 
                   participant_id, study_week, supplement_consumed, butyrate_mM, scfa_notes)
```
This is readable, but can clutter up your workspace (and computer memory) with lots of objects that you have to name and keep track of individually. 

You can also nest functions (i.e., one function inside another). The code chunk below achieves the same result as the one above. The filter function + arguments from the block above are replacing the scfa_filtered dataframe inside the select function. 
```{r}
but_data <- select(filter(exl_df, sample_weight_g > 0.10), 
                   participant_id, study_week, supplement_consumed, butyrate_mM, scfa_notes)
```
This is handy, but can be difficult to read if more than a couple functions are nested, as R evaluates the expression from the inside out; in this case, filtering then selecting.

The last option, pipes, are a recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like `%>%` and are made available via the magrittr package, installed automatically with tidyverse. If you use RStudio, you can type the pipe with the shortcut: Cmd + Shift + M on a Mac (Ctrl + Shift + M on a PC).
```{r}
exl_df %>%
  filter(sample_weight_g > 0.10) %>% 
  filter(scfa_notes != "used average empty weight") %>% 
  select(participant_id, study_week, supplement_consumed, butyrate_mM, scfa_notes)
```

Some may find it helpful to read the pipe like the word "then". In the above code, the pipe sent the exl_df data frame first through filter() to retain rows where the sample weight is greater than 0.10 grams, then through select() to retain only certain columns. Since %>% takes the object on its left and passes it as the first argument to the function on its right, there is no need to explicitly include the data frame as an argument to the filter() and select() functions. The dplyr functions by themselves are somewhat simple, but by combining them into linear workflows with the pipe, more complex manipulations of data frames are accomplished.


# Calculations across columns

### Mutate

Frequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or to find the ratio of values in two columns. For this we’ll use mutate(). To create a new column of weight in kilograms (which is grams / 1000):
```{r}
mutate(exl_df, sample_kg = sample_weight_g / 1000)
```
Scroll through the output, the mutate function adds new columns to the end of the data frames. The sample_kg column just created will be the last column. 

You can also create a second new column based on the first new column within the same call of mutate(). To convert miliMolar (mM) to milimoles (mmol), we have to multiply by the volume of the fecal colletion tubes (which is 0.002 liters). Then divide this by our calculated kilograms (kg), we now have the concentration in mmol/kg: 
```{r}
new_df <- mutate(exl_df, #dataframe to use
       sample_kg = sample_weight_g / 1000, #first column modified with mutate
       acetate_mmol_kg = (acetate_mM*0.002)/sample_kg, #second column modified with mutate
       butyrate_mmol_kg = (butyrate_mM*0.002)/sample_kg )  
```
If this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions too, as long as tidyverse loaded).


# Calculations across rows

Problem: We want to determine the average weekly SCFA concentrations to determine which supplements had a greater butyrogenic effect, however the dataset we are using contains multiple individual measurements per participant. There are multiple possible combinations of functions to complete this task. Many data analysis tasks can be approached using the split-apply-combine paradigm: 

* split the data into groups
* apply some analysis to each group
* combine the results

Depending on the number of groups and type of analysis this can become repetitive, and could make the code quite long. We’ll use the more concise approach using the group_by() function.

### Group + Summarise 

group_by() is often used together with summarize(), which collapses each group into a single-row summary of that group. group_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. summarize() contains the functions and arguments to complete the calculation. So to compute the mean butyrate concentration by semester:
```{r}
new_df %>%
  group_by(semester) %>%
  summarize(mean_butyrate = mean(butyrate_mmol_kg, na.rm = TRUE))
```

However, not all participants consumed the same supplement in a given semester, and some participants did not consume a supplement at all. It is not a meaningful summary statistic to keep these participants together when calculating a mean. Group multiple columns simply by naming all the columns. One row will be returned for each unique combination of variables across the named columns:
```{r}
new_df %>%
  group_by(semester, supplement_consumed) %>%
  summarize(mean_butyrate = mean(butyrate_mmol_kg))
```

You may notice that the mean column just calculated contains NAs. When grouping both by semester and supplement_consumed, there are individuals for which a butyrate measurement could not be determined (this could be due to a concentration below the limit of detection on the HPLC, or there was an issue with the integrity of the sample, or there was an issue with the sample collection protocol). When a group contains an NA the mean function is not able to do the calculation (this is becuase NA is not a number). To avoid this, we change the default behavior of the mean function by adding an argument you've seen already:
```{r}
new_df %>%
  group_by(semester, supplement_consumed) %>%
  summarize(mean_butyrate = mean(butyrate_mmol_kg, na.rm = TRUE))

dim(newest_df)
```

Once these data are grouped, it is possible to summarize multiple variables at the same time, and not necessarily on the same variable (similar to completing multiple calculations within one call of the mutate function). For instance, add a column calculating the standard deviation for each group, and add another column to count the number of observations: 
```{r}
new_df %>%
  group_by(semester, supplement_consumed) %>%
  summarize(mean_butyrate = mean(butyrate_mmol_kg, na.rm = TRUE), 
            sd_butyrate = sd(butyrate_mmol_kg, na.rm = TRUE), # sd() is standard deviation function
            n_butyrate = n()) # n() is a counting function 
```

It is sometimes useful to rearrange the result of a query to inspect the values. For instance, sort on mean_butyrate to put the highest concentrations first:
```{r}
but_data <- new_df %>%
  group_by(semester, supplement_consumed) %>%
  summarize(mean_butyrate = mean(butyrate_mmol_kg, na.rm = TRUE), 
            sd_butyrate = sd(butyrate_mmol_kg, na.rm = TRUE), 
            n_butyrate = n()) %>%
  arrange(-mean_butyrate) #drop "-" to put lowest first 
```
When you use arrange to sort a data frame, it changes the order of the data stored in the global environment. This is different than sorting columns in View(), where the underlying data frame is not altered.


# Export data frame

After curating, extracting information, or summarising raw data, researchers often want to export these curated data sets to save them for future use or to share them with collaborators. Similar to the read_delim() function used for reading CSV or TSV files into R, there is a write_delim() function that generates files from data frames. 

Before using write_delim(), create a new folder, curated_data, in the Lab2 working directory to store this curated dataset; curated datasets should not be stored in the same directory as raw data. It’s good practice to keep them separate. As stated previously, the raw_data folder should only contain the raw, unaltered data. The code in this section will generate the contents of the curated_data directory, so the files curated_data contains can be recreated if necessary. 

```{r}
write_delim(but_data, path = "curated_data/butyrate_data.txt", delim = "\t")
```

Remember the importance of leaving the raw data raw. Our manipulations of the scfa_indv data frame have not altered raw_data/scfa_data.xlsx. Now the cleaned up data frame is ready to share with collaborators or to use in subsequent analyses, and all the code contained in this Rmarkdown document serves as a log of the changes made to the raw data. 


-----
end